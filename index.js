
  var insertCSS = require('insert-css')
  var css = require('sheetify')
  var minidocs = require('minidocs')({"title":"dat","logo":"dat-data.png","contents":{"Introduction":{"Welcome to Dat":"README.md","How Dat works":"how-dat-works.md"},"Ecosystem":{"Overview":"ecosystem.md"},"Specification":{"hyperdrive":"hyperdrive.md","meta.dat":"meta.dat.md"},"References":{"API":"api.md","DIY Dat":"diy-dat.md"}},"markdown":{"api":"## 1.0 Architecture Design\n\n\n  * dat: command-line api\n  * dat-desk: desktop application\n  * hyperdrive: storage layer\n  * discovery-swarm: dat network swarm discovery mechanism\n\n## dat\n\nCommand-line interface for dat\n\n#### `dat share DIR`\n\nCreate a new dat link for the contents of the given directory. Prints a URL, which is a unique public key feed. This public key feed can be appended to. \n\n###### Options\n\n  * `--append=URL`: Adds the new URL to the public key feed.\n  * `--static`: Ensures that the URL cannot be appended to.\n\n#### `dat URL DIR`\n\nDownloads the link to the given directory, and then exits. \n\n###### Options\n\n  * `--seed`: Downloads the link to the given directory and opens up a server that seeds it to the dat peer network.\n  * `--list`: Fetches the metadata for the link and prints out the file list in the console.\n","diy-dat":"# DIY Dat\n\nThis document shows how to write your own compatible `dat` client using node modules.\n\nThe three essential node modules are called [hyperdrive](https://npmjs.org/hyperdrive), [discovery-swarm](https://npmjs.org/discovery-swarm) and [level](https://npmjs.org/level). Hyperdrive does file synchronization and versioning, discovery-swarm does peer discovery over local networks and the Internet, and level provides a local LevelDB for storing metadata. More details are available in [How Dat Works](how-dat-works.md). The [dat](https://npmjs.org/dat) module itself is just some code that combines these modules and wraps them in a command-line API.\n\nHere's the minimal code needed to download data from a dat:\n\n```js\nvar Swarm = require('discovery-swarm')\nvar Hyperdrive = require('hyperdrive')\nvar Level = require('level')\n\n// run this like: node thisfile.js 4c325f7874b4070blahblahetc\n// the dat link someone sent us, we want to download the data from it\nvar link = new Buffer(process.argv[2], 'hex')\n\n// here is the default config dat uses\n// used for MDNS and also as the dns 'app name', you prob shouldnt change this\nvar DAT_DOMAIN = 'dat.local'\n// dat will try this first and pick the first open port if its taken\nvar DEFAULT_LOCAL_PORT = 3282 \n// we run the servers below you can use if you want or run your own\nvar DEFAULT_DISCOVERY = [\n  'discovery1.publicbits.org',\n  'discovery2.publicbits.org'\n]\nvar DEFAULT_BOOTSTRAP = [\n  'bootstrap1.publicbits.org:6881',\n  'bootstrap2.publicbits.org:6881',\n  'bootstrap3.publicbits.org:6881',\n  'bootstrap4.publicbits.org:6881'\n]\n\nvar db = Level('./dat.db')\nvar drive = Hyperdrive(db)\nvar swarm = Swarm({\n  id: drive.core.id,\n  dns: {server: DEFAULT_DISCOVERY, domain: DAT_DOMAIN},\n  dht: {bootstrap: DEFAULT_BOOTSTRAP},\n  stream: function () {\n    // this is how the swarm and hyperdrive interface\n    console.log('new peer stream')\n    return drive.createPeerStream()\n  }\n})\n\nswarm.once('listening', function () {\n  console.log('joining', link)\n  // join the swarm\n  swarm.join(new Buffer(link, 'hex'))\n  // tell hyperdrive to start downloading/uploading in ./data\n  var archive = drive.get(link, process.cwd() + '/data')\n  archive.ready(function (err) {\n    console.log('archive ready')\n    // a stream of all metadata. after retrieving each entry metadata will be cached locally\n    // but the first time it has to fetch it from the swarm\n    var metadata = archive.createEntryStream()\n    // start downloading all entries, or choose your own filter logic to download specific entries\n    // entries will be either files or directories\n    metadata.on('data', function (entry) {\n      var dl = archive.download(entry)\n      console.log('downloading', entry.name, dl)\n      \n      dl.on('ready', function () {\n        console.log('download started', entry.name, dl)\n      })\n    })\n  })\n})\n\nswarm.listen(DEFAULT_LOCAL_PORT)\n```\n","ecosystem":"If you want to go deeper and see the implementations we are using in the [Dat command-line tool](https://github.com/maxogden/dat), here you go:\n\n- [dat](https://www.npmjs.com/package/dat) - the main command line tool that uses all of the below\n- [discovery-channel](https://www.npmjs.com/package/discovery-channel) - discover data sources\n- [discovery-swarm](https://www.npmjs.com/package/discovery-swarm) - discover and connect to sources\n- [hyperdrive](https://www.npmjs.com/package/hyperdrive) - The file sharing network dat uses to distribute files and data. A technical specification / discussion on how hyperdrive works is [available here](https://github.com/mafintosh/hyperdrive/blob/master/SPECIFICATION.md)\n- [hypercore](https://www.npmjs.com/package/hypercore) - exchange low-level binary blocks with many sources\n- [bittorrent-dht](https://www.npmjs.com/package/bittorrent-dht) - use the Kademlia Mainline DHT to discover sources\n- [dns-discovery](https://www.npmjs.com/package/dns-discovery) - use DNS name servers and Multicast DNS to discover sources\n- [utp-native](https://www.npmjs.com/package/utp-native) - UTP protocol implementation\n- [rabin](https://www.npmjs.com/package/rabin) - Rabin fingerprinter stream\n- [merkle-tree-stream](https://www.npmjs.com/package/merkle-tree-stream) - Used to construct Merkle trees from chunks\n","how-dat-works":"# How Dat Works\n\nNote this is about Dat 1.0 and later. For historical info about earlier incarnations of Dat (Alpha, Beta) check out [this post](http://dat-data.com/blog/2016-01-19-brief-history-of-dat).\n\nWhen someone starts downloading data with the [Dat command-line tool](https://github.com/maxogden/dat), here's what happens:\n\n## Phase 1: Source discovery\n\nDat links look like this: `dat.land/c3fcbcdcf03360529b47df32ccfb9bc1d7f64aaaa41cca43ca9ac7f6778db8da`. The domain, dat.land, is there so if someone opens the link in a browser we can provide them with download instructions, and as an easy way for people to visually distinguish and remember Dat links. Dat itself doesn't actually use the dat.land part, it just needs the last part of the link which is a fingerprint of the data that is being shared. The first thing that happens when you go to download data using one of these links is you ask various discovery networks if they can tell you where to find sources that have a copy of the data you need.\n\nSource discovery means finding the IP and port of all the known data sources online that have a copy of that data you are looking for. You can then connect to them and begin exchanging data. By introducing this discovery phase we are able to create a network where data can be discovered even if the original data source disappears.\n\nThe discovery protocols we use are [DNS name servers](https://en.wikipedia.org/wiki/Name_server), [Multicast DNS](https://en.wikipedia.org/wiki/Multicast_DNS) and the [Kademlia Mainline Distributed Hash Table](https://en.wikipedia.org/wiki/Mainline_DHT) (DHT). Each one has pros and cons, so we combine all three to increase the speed and reliability of discovering data sources.\n\nWe run a [custom DNS server](https://www.npmjs.com/package/dns-discovery) that Dat clients use (in addition to specifying their own if they need to), as well as a [DHT bootstrap](https://github.com/bittorrent/bootstrap-dht) server. These discovery servers are the only centralized infrastructure we need for Dat to work over the Internet, but they are redundant, interchangeable, never see the actual data being shared, and anyone can run their own and Dat will still work even if they all go down. If this happens discovery will just be manual (e.g. manually sharing IP/ports). Every data source that has a copy of the data also advertises themselves across these discovery networks.\n\nThe discovery logic itself is handled by a module that we wrote called [discovery-channel](http://npmjs.org/discovery-channel), which wraps other modules we wrote to implement DNS and DHT logic into a single interface. We can give the Dat link we want to download to discovery-channel and we will get back all the sources it finds across the various discovery networks.\n\n## Phase 2: Source connections\n\nUp until this point we have just done searches to find who has the data we need. Now that we know who should talk to, we have to connect to them. We use either [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) or [UTP](https://en.wikipedia.org/wiki/Micro_Transport_Protocol) sockets for the actual peer to peer connections. UTP is nice because it is designed to *not* take up all available bandwidth on a network (e.g. so that other people sharing your wifi can still use the Internet). We then layer on our own file sharing protocol on top, called [Hypercore](https://github.com/mafintosh/hypercore). We also are working on WebRTC support so we can incorporate Browser and Electron clients for some really open web use cases.\n\nWhen we get the IP and port for a potential source we try to connect using all available protocols (currently TCP and sometimes UTP) and hope one works. If one connects first, we abort the other ones. If none connect, we try again until we decide that source is offline or unavailable to use and we stop trying to connect to them. Sources we are able to connect to go into a list of known good sources, so that if our Internet connection goes down we can use that list to reconnect to our good sources again quickly.\n\nIf we get a lot of potential sources we pick a handful at random to try and connect to and keep the rest around as additional sources to use later in case we decide we need more sources. A lot of these are parameters that we can tune for different scenarios later, but have started with some best guesses as defaults.\n\nThe connection logic is implemented in a module called [discovery-swarm](https://www.npmjs.com/package/discovery-swarm). This builds on discovery-channel and adds connection establishment, management and statistics. You can see stats like how many sources are currently connected, how many good and bad behaving sources you've talked to, and it automatically handles connecting and reconnecting to sources for you. Our UTP support is implemented in the module [utp-native](https://www.npmjs.com/package/utp-native).\n\n## Phase 3: Data exchange\n\nSo now we have found data sources, have connected to them, but we havent yet figured out if they *actually* have the data we need. This is where our file transfer protocol [Hyperdrive](https://www.npmjs.com/package/hyperdrive) comes in.\n\nThe short version of how Hyperdrive works is: It breaks file contents up in to pieces, hashes each piece and then constructs a [Merkle tree](https://en.wikipedia.org/wiki/Merkle_tree) out of all of the pieces. This ultimately gives us the Dat link, which is the top level hash of the Merkle tree.\n\nHere's the long version:\n\nHyperdrive shares and synchronizes a set of files, similar to rsync or Dropbox. For each file in the drive we use a technique called Rabin fingerprinting to break the file up into pieces. Rabin fingerprints are a specific strategy for what is called Content Defined Chunking. Here's an example:\n\n![cdc diagram](https://raw.githubusercontent.com/datproject/docs/master/meta/cdc.png)\n\nWe have configured our Rabin chunker to produce chunks that are around 16KB on average. So if you share a folder containing a single 1MB JPG you will get around 64 chunks.\n\nAfter feeding the file contents through the chunker, we take the chunks and calculate the SHA256 hash of each one. We then arrange these hashes into a special data structure we developed that we call the Flat In-Order Merkle Tree.\n\n### Flat In-Order Merkle Tree\n\n```\n      3\n  1       5\n0   2   4   6\n```\n\nWant to go lower level? Check out [How Hypercore Works](hyperdrive.md#how-hypercore-works)\n\nWhen two peers connect to each other and begin speaking the Hyperdrive protocol they can efficiently determine if they have chunks the other one wants, and begin exchanging those chunks directly. Hyperdrive gives us the flexibility to have random access to any portion of a file while still verifying the other side isnt sending us bad data. We can also download different sections of files in parallel across all of the sources simultaneously, which increases overall download speed dramatically.\n\n## Phase 4: Data archiving\n\nSo now that you've discovered, connected, and downloaded a copy of some data you can stick around for a while and serve up copies of the data to others who come along and want to download it.\n\nThe first phase, source discovery, is actually an ongoing process. When you first search for data sources you only get the sources available at the time you did your search, so we make sure to perform discovery searches as often is practically possible to make sure new sources can be found and connected to.\n\nEvery user of Dat is a source as long as they have 1 or more chunks of data. Just like with other decentralized file sharing protocols you will notice Dat may start uploading data before it finishes downloading.\n\nIf the original source who shared the data goes offline it's OK, as long as other sources are available. As part of the mission as a not-for-profit we will be working with various institutions to ensure there are always sources available to accept new copies of data and stay online to serve those copies for important datasets such as scientific research data, open government data etc.\n\nBecause Dat is built on a foundation of strong cryptographic data integrity and content addressable storage it gives us the possibility of implementing some really interesting version control techniques in the future. In that scenario archival data sources could choose to offer more disk space and archive every version of a Dat repository, whereas normal Dat users might only download and share one version that they happen to be interested in.\n\n## Implementations\n\nThis covered a lot of ground. If you want to go deeper and see the implementations we are using in the [Dat command-line tool](https://github.com/maxogden/dat), go to the [Dependencies](ecosystem.md) page\n","hyperdrive":"# Hyperdrive + Hypercore Specification\n\n## DRAFT Version 1\n\nHyperdrive is the peer-to-peer data distribution protocol that powers Dat. It consists of two parts. First there is hypercore which is the core protocol and swarm that handles distributing append-only logs of any binary data. The second part is hyperdrive which adds a filesystem specific protocol on top of hypercore.\n\n## Hypercore\n\nThe goal of hypercore is to distribute append-only logs across a network of peers. Peers download parts of the logs from other peers and can choose to only download the parts of a log they care about. Logs can contain arbitrary binary data payloads.\n\nA core goal is to be as simple and pragmatic as possible. This allows for easier implementations of clients which is an often overlooked property when implementing distributed systems. First class browser support is also an important goal as p2p data sharing in browsers is becoming more viable every day as WebRTC matures.\n\nIt also tries to be modular and export responsibilities to external modules whenever possible. Peer discovery is a good example of this as it handled by 3rd party modules that wasn't written with hyperdrive in mind. A benefit of this is a much smaller core implementation that can focus on smaller and simpler problems.\n\nPrioritized synchronization of parts of a feed is also at the heart of hyperdrive as this allows for fast streaming with low latency of data such as structured datasets (wikipedia, genomic datasets), linux containers, audio, videos, and much more. To allow for low latency streaming another goal is also to keep verifiable block sizes as small as possible - even with huge data feeds.\n\nThe protocol itself draws heavy inspiration from existing file sharing systems such as BitTorrent and [PPSP](https://datatracker.ietf.org/doc/rfc7574/?include_text=1)\n\n## How Hypercore works\n\n### Flat In-Order Trees\n\nA Flat In-Order Tree is a simple way represent a binary tree as a list. It also allows you to identify every node of a binary tree with a numeric index. Both of these properties makes it useful in distributed applications to simplify wire protocols that uses tree structures.\n\nFlat trees are described in [PPSP RFC 7574 as \"Bin numbers\"](https://datatracker.ietf.org/doc/rfc7574/?include_text=1) and a node version is available through the [flat-tree](https://github.com/mafintosh/flat-tree) module.\n\nA sample flat tree spanning 4 blocks of data looks like this:\n\n```\n0\n  1\n2\n    3\n4\n  5\n6\n```\n\nThe even numbered entries represent data blocks (leaf nodes) and odd numbered entries represent parent nodes that have two children.\n\nThe depth of an tree node can be calculated by counting the number of trailing 1s a node has in binary notation.\n\n```\n5 in binary = 101 (one trailing 1)\n3 in binary = 011 (two trailing 1s)\n4 in binary = 100 (zero trailing 1s)\n```\n\n1 is the parent of (0, 2), 5 is the parent of (4, 6), and 3 is the parent of (1, 5).\n\nIf the number of leaf nodes is a power of 2 the flat tree will only have a single root.\n\nOtherwise it'll have more than one. As an example here is a tree with 6 leafs:\n\n```\n0\n  1\n2\n    3\n4\n  5\n6\n\n8\n  9\n10\n```\n\nThe roots spanning all the above leafs are 3 an 9. Throughout this document we'll use following tree terminology:\n\n* `parent` - a node that has two children (odd numbered)\n* `leaf` - a node with no children (even numbered)\n* `sibling` - the other node with whom a node has a mutual parent\n* `uncle` - a parent's sibling\n\n## Merkle Trees\n\nA merkle tree is a binary tree where every leaf is a hash of a data block and every parent is the hash of both of its children.\n\nMerkle trees are useful for ensuring the integrity of content.\n\nLet's look at an example. Assume we have 4 data blocks, `(a, b, c, d)` and let `h(x)` be a hash function (the hyperdrive stack uses sha256 per default).\n\nUsing flat-tree notation the merkle tree spanning these data blocks looks like this:\n\n```\n0 = h(a)\n  1 = h(0 + 2)\n2 = h(b)\n    3 = h(1 + 5)\n4 = h(c)\n  5 = h(4 + 6)\n6 = h(d)\n```\n\nAn interesting property of merkle trees is that the node 3 hashes the entire data set. Therefore we only need to trust node 3 to verify all data. However as we learned above there will only be a single root if there is a power of two data blocks.\n\nAgain lets expand our data set to contain 6 items `(a, b, c, d, e, f)`:\n\n```\n0 = h(a)\n  1 = h(0 + 2)\n2 = h(b)\n    3 = h(1 + 5)\n4 = h(c)\n  5 = h(4 + 6)\n6 = h(d)\n\n8 = h(e)\n  9 = h(8 + 10)\n10 = h(f)\n```\n\nTo ensure always have only a single root we'll simply hash all the roots together again. At most there will be `log2(number of data blocks)`.\n\nIn addition to hashing the roots we'll also include a bin endian uint64 binary representation of the corresponding node index.\n\nUsing the two above examples the final hashes would be:\n\n```\nhash1 = h(uint64be(#3) + 3)\nhash2 = h(uint64be(#9) + 9 + uint64be(#3) + 3)\n```\n\nEach of these hashes can be used to fully verify each of the trees. Let's look at another example. Assume we trust `hash1` and another person wants to send block `0` to us. To verify block `0` the other person would also have to send the sibling hash and uncles until it reaches a root and the other missing root hashes. For the first tree that would mean hashes `(2, 5)`.\n\nUsing these hashes we can reproduce `hash1` in the following way:\n\n```\n0 = h(block received)\n  1 = h(0 + 2)\n2 = (hash received)\n    3 = h(1 + 5)\n  5 = (hash received)\n```\n\nIf `h(uint64be(#3) + 3) == hash1` then we know that data we received from the other person is correct. They sent us `a` and the corresponding hashes.\n\nSince we only need uncle hashes to verify the block the amount of hashes we need is at worst `log2(number-of-blocks)` and the roots of the merkle trees which has the same complexity.\n\nA merkle tree generator is available on npm through the [merkle-tree-stream](https://github.com/mafintosh/merkle-tree-stream) module.\n\n## Merkle Tree Deduplication\n\nMerkle trees have another great property. They make it easy to deduplicate content that is similar.\n\nAssume we have two similar datasets:\n\n```\n(a, b, c, d, e)\n(a, b, c, d, f)\n```\n\nThese two datasets are the same except their last element is different. When generating merkle trees for the two data sets you'd get two different root hashes out.\n\nHowever if we look a the flat-tree notation for the two trees:\n\n```\n0\n  1\n2\n    3\n4\n  5\n6\n\n8\n```\n\nWe'll notice that the hash stored at 3 will be the same for both trees since the first four blocks are the same. Since we also send uncle hashes when sending a block of data we'll receive the hash for 3 when we request any block. If we maintain a simple index that maps a hash into the range of data it covers we can detect that we already have the data spanning 3 and we won't need to re-download that from another person.\n\n```\n1 -> (a, b)\n3 -> (a, b, c, d)\n5 -> (c, d)\n```\n\nThis means that two datasets share a similar sequence of data the merkle tree helps you detect that.\n\n## Signed Merkle Trees\n\nAs described above the top hash of a merkle tree is the hash of all its content. This has both advantages and disadvanteges.\n\nAn advantage is that you can always reproduce a merkle tree simply by having the data contents of a merkle tree.\n\nA disadvantage is every time you add content to your data set your merkle tree hash changes and you'll need to re-distribute the new hash.\n\nUsing a bit of cryptography however we can make our merkle tree appendable. First generate a cryptographic key pair that can be used to sign data using [ed25519](https://ed25519.cr.yp.to/) keys, as they are compact in size (32 byte public keys). A key pair (public key, secret key) can be used to sign data. Signing data means that if you trust a public key and you receive data and a signature for that data you can verify that a signature was generated with the corresponding secret key.\n\nHow does this relate to merkle trees? Instead of distributing the hash of a merkle tree we can distribute our public key instead. We then use our secret key to continously sign the merkle trees of our data set every time we append to it.\n\nAssume we have a data set with only a single item in it `(a)` and a key pair `(secret, public)`:\n\n```\n(a)\n```\n\nWe generate a merkle tree for this data set which will have the roots `0` and sign the hash of these roots (see the merkle tree section) with our secret key.\n\nIf we want to send `a` to another person and they trust our public key we simply send `a` and the uncles needed to generate the roots plus our signature.\n\nIf we append a new item to our data set we simply do the same thing:\n\n```\n(a, b)\n```\n\nNotice that all new signatures verify the entire dataset since they all sign a merkle tree that spans all data. This serves two purposes. First of all it makes sure that the dataset publisher cannot change old data. It also ensures that the publisher cannot share different versions of the same dataset to different persons without the other people noticing it (at some point they'll get a signature for the same node index that has different hashes if they talk to multiple people).\n\nThis technique has the added benefit that you can always convert a signed merkle tree to a normal unsigned one if you wish (or turn an unsigned tree into a signed tree).\n\nIn general you should send as wide as possible signed tree back when using signed merkle trees as that lowers the amount of signatures the other person needs to verify which has a positive performance impact for some platforms. It will also allow other users to more quickly detect if a tree has duplicated content.\n\n## Block Tree Digest\n\nWhen asking for a block of data we want to reduce the amount of duplicate hashes that are sent back.\n\nIn the merkle tree example for from earlier we ended up sending two hashes `(2, 5)` to verify block `0`.\n\n```\n// If we trust 3 then 2 and 5 are needed to verify 0\n\n0\n  1\n2\n    3\n4\n  5\n6\n```\n\nNow if we ask for block `1` afterwards (`2` in flat tree notation) the other person doesn't need to send us any new hashes since we already received the hash for `2` when fetching block `0`.\n\nIf we only use non-signed merkle trees the other person can easily calculate which hashes we already have if we tell them which blocks we've got.\n\nThis however isn't always possible if we use a signed merkle tree since the roots are changing. In general it also useful to be able to communicate that you have some hashes already without disclosing all the blocks you have.\n\nTo communicate which hashes we have just have to communicate two things: which uncles we have and whether or not we have any parent node that can verify the tree.\n\nLooking at the above tree that means if we want to fetch block `0` we need to communicate whether of not we already have the uncles `(2, 5)` and the parent `3`. This information can be compressed into very small bit vector using the following scheme.\n\nLet the trailing bit donate whether or not the leading bit is a parent and not a uncle. Let the previous trailing bits denote wheather or not we have the next uncle.\n\nFor example for block `0` the following bit vector `1011` is decoded the following way\n\n```\n// for block 0\n\n101(1) <-- tell us that the last bit is a parent and not an uncle\n10(1)1 <-- we already have the first uncle, 2 so don't send us that\n1(0)11 <-- we don't have the next uncle, 5\n(1)000 <-- the final bit so this is parent. we have the next parent, 3\n```\n\nSo using this digest the person can easily figure out that they only need to send us one hash, `5`, for us to verify block `0`.\n\nThe bit vector `1` (only contains a single one) means that we already have all the hashes we need so just send us the block.\n\nThese digests are very compact in size, only `(log2(number-of-blocks) + 2) / 8` bytes needed in the worst case. For example if you are sharing one trillion blocks of data the digest would be `(log2(1000000000000) + 2) / 8 ~= 6` bytes long.\n\n### Bitfield Run length Encoding\n\n(talk about rle)\n\n### Basic Privacy\n\n(talk about the privacy features + discovery key here)\n\n## Hypercore Feeds\n\n(talk about how we use the above concepts to create a feed of data)\n\n## Hypercore Replication Protocol\n\nLets assume two peers have the identifier for a hypercore feed. This could either be the hash of the merkle tree roots described above or a public key if they want to share a signed merkle tree. The two peers wants to exchange the data verified by this tree. Lets assume the two peers have somehow connected to each other.\n\nHypercore uses a message based protocol to exchange data. All messages sent are encoded to binary values using Protocol Buffers. Protocol Buffers are a widely supported schema based encoding support. A Protocol Buffers implementation is available on npm through the [protocol-buffers](https://github.com/mafintosh/protocol-buffers) module.\n\nThese are the types of messages the peers send to each other\n\n#### Open\n\nThis should be the first message sent and is also the only message without a type. It looks like this\n\n``` protobuf\nmessage Open {\n  required bytes feed = 1;\n  required bytes nonce = 2;\n}\n```\n\nThe `feed` should be set to the discovery key of the Merkle Tree as specified above. The `nonce` should be set to 24 bytes of high entropy random data. When running in encrypted mode this is the only message sent unencrypted.\n\nWhen you are done using a channel send an empty message to indicate end-of-channel.\n\n#### `0` Handshake\n\nThe message contains the protocol handshake. It has type `0`.\n\n``` protobuf\nmessage Handshake {\n  required bytes id = 1;\n  repeated string extensions = 2;\n}\n```\n\nYou should send this message after sending an open message. By sending it after an open message it will be encrypted and we wont expose our peer id to a third party. The current protocol version is 0.\n\n#### `1` Have\n\nYou can send a have message to give the other peer information about which blocks of data you have. It has type `1`.\n\n``` protobuf\nmessage Have {\n  required uint64 start = 1;\n  optional uint64 end = 2;\n  optional bytes bitfield = 3;\n}\n```\n\nIf using a bitfield it should be encoded using a run length encoding described above. It is a good idea to send a have message soon as possible if you have blocks to share to reduce latency.\n\n#### `2` Want\n\nYou can send a have message to give the other peer information about which blocks of data you want to have. It has type `2`.\n\n``` protobuf\nmessage Want {\n  required uint64 start = 1;\n  optional uint64 end = 2;\n}\n```\n\nYou should only send the want message if you are interested in a section of the feed that the other peer has not told you about.\n\n#### `3` Request\n\nSend this message to request a block of data. You can request a block by block index or byte offset. If you are only interested\nin the hash of a block you can set the hash property to true. The nodes property can be set to a tree digest of the tree nodes you already\nhave for this block or byte range. A request message has type `3`.\n\n``` protobuf\nmessage Request {\n  optional uint64 block = 1;\n  optional uint64 bytes = 2;\n  optional bool hash = 3;\n  optional uint64 nodes = 4;\n}\n```\n\n#### `4` Data\n\nSend a block of data to the other peer. You can use this message to reply to a request or optimistically send other blocks of data to the other client. It has type `4`.\n\n``` protobuf\nmessage Data {\n  message Node {\n    required uint64 index = 1;\n    required uint64 size = 2;\n    required bytes hash = 3;\n  }\n\n  required uint64 block = 1;\n  optional bytes value = 2;\n  repeated Node nodes = 3;\n  optional bytes signature = 4;\n}\n````\n\n#### `5` Cancel\n\nCancel a previous sent request. It has type `5`.\n\n``` protobuf\nmessage Cancel {\n  optional uint64 block = 1;\n  optional uint64 bytes = 2;\n}\n```\n\n#### `6` Pause\n\nAn empty message that tells the other peer that they should stop requesting new blocks of data. It has type `6`.\n\n#### `7` Resume\n\nAn empty message that tells the other peer that they can continue requesting new blocks of data. It has type `7`.\n","meta.dat":"# meta.dat\n\nDat uses a simple metadata file called `meta.dat`. The purpose of this file is to store the fingerprints of the files in a Dat repository. If you create a `meta.dat` file for a set of files, you can host it on a static HTTP server along with the files and Dat clients will be able to download and verify your files, even if you aren't running a Dat server!\n\n# File format\n\n```\n<Header><Entries Index...><Entries...>\n```\n\nThe format is a header followed by an index of many entries. Entry order is based on the indexing determined by the [Flat In-Order Tree](hyperdrive.md#flat-in-order-trees) algorithm we use in Dat. After the entry index, a concatinated list of entries follows.\n\n### Header format\n\n```\n<varint header-length><header protobuf>\n```\n\nThe header protobuf has this schema:\n\n``` protobuf\nmessage Header {\n  required bytes datLink = 1;\n  required uint64 entries = 2;\n  optional bool isSigned = 3;\n  optional string hashType = 4 [default = \"sha256\"];\n  optional uint32 hashLength = 5 [default = 32];\n  optional string signatureType = 6 [default = \"ed25519\"];\n  optional uint32 signatureLength = 7 [default = 64];\n}\n```\n\n### Entry index format\n\nFor non-signed entries:\n\n```\n<8-byte-chunk-end><chunk-hash>\n```\n\nThe 8-byte-chunk-end is an unsigned big endian 64 bit integer that should be the absolute position in the file for the **end of the chunk**.\n\nFor signed entries in live feeds (only applies to even numbered nodes e.g. leaf nodes):\n\n```\n<8-byte-chunk-end><chunk-signature><chunk-hash>\n```\n\nFor any odd nodes, in either a live or a non-live feed, the non-signed entry format will be used.\n\n## Example\n\nGiven a tree like this you might want to look up in a `meta.dat` file the metadata for a specific node:\n\n```\n0─┐  \n  1─┐\n2─┘ │\n    3\n4─┐ │\n  5─┘\n6─┘\n```\n\nIf you wanted to look up the metadata for 3, you could read the third (or any!) entry from meta.dat:\n\nFirst you have to read the varint at the beginning of the file so you know how big the header is:\n\n``` js\nvar varint = require('varint') // https://github.com/chrisdickinson/varint\nvar headerLength = varint.decode(firstChunkOfFile)\n```\n\nNow you can read the header from the file\n\n``` js\nvar headerOffset = varint.encodingLength(headerLength)\nvar headerEndOffset = headerOffset + headerLength\nvar headerBytes = firstChunkOfFile.slice(headerOffset, headerEndOffset)\n```\n\nTo decode the header use the protobuf schema. We can use the [protocol-buffers](https://github.com/mafintosh/protocol-buffers) module to do that.\n\n``` js\nvar messages = require('protocol-buffers')(fs.readFileSync('meta.dat.proto'))\nvar header = messages.Header.decode(headerBytes)\n```\n\nNow we have all the configuration required to calculate an entry offset.\n\n``` js\nvar entryNumber = 42\nvar entryOffset = headerEndOffset + entryNumber * (8 + header.hashLength)\n```\n\nIf you have a signed feed, you have to take into account the extra space required for the signatures in the even nodes.\n\n``` js\nvar entryOffset = headerLength + entryNumber * (8 + header.hashLength)\n                  + Math.floor(entryNumber / 2) * header.signatureLength\n```\n","README":"# Dat 1.0 docs\n\nDocumentation resources for [dat](https://github.com/maxogden/dat) 1.0 release candidate 1 and the surrounding ecosystem.\n\n## Installation\n\n```\nnpm install -g dat\n```\n\n## Sharing data\n\nDat is a peer to peer file sharing tool. To share data, first `cd` into a directory you want to share, and type:\n\n```\n$ dat link\n```\n\nThis will create a link, that looks like `dat://...`. This is a unique hash, generated by the contents of the files (including those in subdirectories) inside the current directory. Dat ignores hidden files.\n\nYour output might look something like this:\n\n```\n$ dat link\nScanning folder, found 98 files in 5 directories. 47.12 MB total.\ndat://a9933c3d00e1134e5814a0fe2b0f1166885f523dfe0d135a39a2ca4b43840d83\nServing data (1 connection(s))\n```\n\nOn another computer, I can type:\n\n```\ndat dat://a9933c3d00e1134e5814a0fe2b0f1166885f523dfe0d135a39a2ca4b43840d83\n```\n\nAnd the files will be downloaded inside of the current directory. The process will stay open, re-hosting the data to ensure redundancy of the data.\n\n## Versioning\n\nIf you change any file inside the directory, you will get a different link. Each link is unique to the file list and data contents inside each file.\n\nEverything about the filesystem is replicated between two dat hosts, including directory structure, file modes, among other filesystem metadata. For example, changing the file mode of a single file will create an entirely different link.\n\n## Peer Discovery\n\nDat uses a variety of different methods to discover peers that have the data it's looking for, including DNS, Multicast DNS, UDP, and TCP. See [discovery-swarm](https://github.com/mafintosh/discovery-swarm) for more information.\n\n## Local storage\n\nDat stores its data in a hidden folder that is stored by default in the user's home directory.\n\n```\n~/.dat\n```\nThe global `.dat` folder has the following contents:\n\n```\n$ ls ~/.dat\ndb\nconfig.json\n```\n"},"initial":"api"})
  css('/Users/chia-liang.kao/src/st/stconfig/git/dat/docs/styles.css', { global: true })
  minidocs.start('#choo-root')
  